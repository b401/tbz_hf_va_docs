= Vorstudie: Anomalie Detection in Netzwerken
Ives Schneider <ives.schneider@i-401.xyz>
:doctype: pdf
:author: Ives Schneider
:subtitle: Anomalie Detection in Netzwerken
:ntitle: Vorstudie: {subtitle}
:imagesdir: ./images
:class: ITSE17a
:pdf-stylesdir: ./resources/themes
:pdf-fontsdir: ./resources/fonts
:pdf-style: tbz
:allow-uri-read:
:sectnums:
:toc:
:toc-title: Index
:title-page:

<<<

== Management Summary
Durchschnittlich dauert es 206 Tage, bis eine Anomalie in Firmennetzwerken entdeckt werden. Studien zeigen, umso länger eine Anomalie unentdeckt bleibt umso teurer werden die Auswirkungen. https://www.ibm.com/security/data-breach[IBM] +
{nbsp} +
Anomalien können von kleineren Netzwerkunterbrüchen bis hin zum Totalausfall mehrerer geschäftswichtigen Bussiness Applikationen und Infrastrukturen führen. Innerhalb dieser Vorstudie, wird das POC Netz welches für die Entwicklung einer Anomalie Erkennung verwendet wird, näher definiert (Umsysteme), sowie die Möglichkeit angeschaut, in wieweit Anomalien bereits erkannt werden können. +
Anhand der bereits vorhandenen Infrastruktur kann eine relativ gute Annahme gemacht werden, welche Umsysteme angesprochen werden können.
{nbsp} +
Für die fehlende Software wurde eine Evaluation anhand einer Gegenüberstellung gemacht. +

.Log Collector System
- Graylog
- ELK-Stack
- Splunk

.Network Monitoring System
- PRTG
- Nagios

Anhand der Evaluation wurde entschieden, dass das Projekt mit Splunk as Log Collector und PRTG als Network Monitoring System durchgeführt werden kann. +
Tiefere Anschauung zum Umfang und Limitationen der Software wird in der Hauptstudie weiterverfolgt. +


<<<

== Projektinformationen

{nbsp} +
{nbsp} +

.*Auftraggeber*
Technische Berufsschule Zürich +
Sihlquai 101 +
8090 Zürich +
admin.hf@tbz.zh.ch

{nbsp} +

.*Projektleitung*
Ives Schneider +
Binzstrasse 19 +
8712 Stäfa +
ives.schneider@i-401.xyz

{nbsp} +

.*Experte*
Marco Sieber +
marco.sieber@tbz.ch

{nbsp} +

.*Studiengang*
IT Service Engineer HF

.*Klasse*
ITSE17a

<<<

=== Projektauftrag

==== Projektbeschreibung
Basierend auf dem Netzwerk der TBZ HF soll eine Lösung zur Anomalie Detection evaluiert werden und in einem POC aufgebaut und entsprechend dokumentiert werden (inkl. Betriebsdokumentationen). +
Die Lösung soll die Fähigkeit bieten, unautorisierte Aktivitäten im Netzwerk erkennen zu können.

==== Problemstellung
Netzwerkanomalien können weitreichende folgen haben. Angefangen von kleineren Paketverlusten, bis hin zum Ausfall ganzer Komponenten. +
Die Erkennung solcher Anomalien ist meist mühselig und schwierig und kann unter Umständen mehrere Ressourcen aufbrauchen bis der Standard wiederhergestellt werden kann.

==== Projektorganisation

.Projekt Mitarbeiter

image::organigram.png[Projekt Mitarbeiter,align="center",width=400px]

<<<

==== Projektzeitplan

TODO

<<<


== Projektziele

Die Anforderungen sollen die nötigen Funktionen erfüllen, um eine gute Übersicht über das bestehende Netzwerk aufzuzeigen, Anomalien erkennen und deuten zu könenn.

=== Muss Kriterien

* Baseline muss umfänglich ersichtlich sein
** _Es muss eine grundlegende Analyse des normalen Netzwerkverkehrs erstellt werden._
* Lösung soll Modular sein
** _Die Lösung soll mit mehreren NMS sowie Switches arbeiten können._
** _Falls ein "Modul" nicht erwünscht sein sollte, kann es deaktiviert werden._
* Scaleability soll vorhanden sein
** _Es soll die Möglichkeit bieten mehrere Log-Collectoren anzuschliessen._
* OpenSource Lösung
** _Der Sourcecode soll für Erweiterungen veröffentlicht werden._
* Betriebsdokumentation
** _Der Betrieb kann klar anhand einer Dokumentation nachvollzogen und nachgestellt werden._
* Installationsdokumentation
** _Es ist eine Installationsdokumentation vorhanden, um Installationen ohne Experten nachzustellen._
* Alerts müssen gemeldet werden
** _Anomalien werden via Mail an die zuständigen Administratoren gemeldet._
* Web oder CLI Interface
** _Es ist ein Web und/oder CLI Interface vorhanden um die Applikation zu managen._

=== Kann Kriterien

* Push Benachrichtigungen
** _Alerts können via Push notification abgesendet/empfangen werden._
* SSO Authentifizierung
** _Single-Sign-On Anbindung an bestehende SSO Lösungen._
* Multiarch
** _Die Lösung soll sowohl unter x86 sowie x86_64 laufen._
* OS Independent
** _Die Lösung soll keine Abhängigkeit des unterliegenden Betriebssystem haben._

<<<

=== Bewertungskriterien

* API
** _Die Applikation besitzt ein ausführliches und umfassendes API._
* Dokumentation
** _API sowie andere Konfigurationsmöglichkeiten sind ausführlich dokumentiert._
* Performance
** _Ein single-node muss auch bei höheren Lasten noch immer eine gute Performance liefern._
* Skalierbarkeit
** _Clusterfunktionalität der Applikation._
* Lizenz
** _Wie offen ist die Lizenz, kann die Applikation geändert werden?_
* Ausgereift
** Wie ausgereift ist die Applikation?
* Aktivität
** Wie aktiv wird an der Applikation weiterentwickelt?

=== Gewichtung

Die Gewichtung kann für beide evaluierten Software Lösungen eingesetzt werden.

image::praeferenzmatrix.png[Präferenzmatrix,640,480]

<<<

== Analysetechnik

=== Abgrenzung
Um Anomalien frühzeitig und um möglichst "False Positives" zu vermeiden, ist es unabdingbar, möglichst viele Netzwerkgeräte miteinzubeziehen. Prozesse sowie bereits vorhandene Richtlinien werden durch das Projekt nicht abgeändert, sondern eventuell noch durch neue Prozesse ergänzt.

=== Systemgrenzen
image::ei.png[Systemgrenzen,640,480]

==== Einflussgrössen / Restriktionen

Da das gesamte Netzwerk überwacht und Daten analysiert werden müssen, ist es wichtig, dass das Projekt transparent und mit den gegebenen Datenschutzrichtlinien im Einklang durchgeführt wird. +
Grundsätzlich sollen keine bestehenden Arbeitsabläufe geändert oder gefährdet werden.

<<<

=== Analyse der Teilsysteme

==== Clients
Vorhandene Clients müssen kategorisiert und eingestuft werden. +
Sobald ein Client hinzukommen oder entfernt werden sollte, muss eine Reaktion darauf ausgelöst werden und die Änderung in der Baseline vermerkt werden.

==== Server
Unbekannte Server und Services müssen gemeldet und eingestuft werden. +
Ebenfalls sollten unbekannte Server direkt eine Reaktion auslösen.

==== Switches
Switches werden anhand von SNMP überwacht. Sollte sich ein Wert ausserhalb der Baseline befinden, muss eine Reaktion darauf erfolgen.

==== Firewall
Firewall werden mithilfe APIs abgefragt. Falls kein API vorhanden sein sollte, wird auf SNMP zurückgegriffen.

==== NMS
Das NMS wird als einer der Hauptquellen für Informationen über das momentane Verhalten des Netzwerkes zu Rate gezogen. +
Ebenfalls wird das NMS passiv sowie pro-aktiv in die Informationssuche mit eingschlossen.

==== AccessPoints
Es soll stets eine Übersicht über die Anzahl der verbundenen Clients vorhanden sein.

==== Router
Router werden anhand ARP sowie SNMP in die Überwachung mit eingebunden.

==== Logs
Es wird ein Log-Collector evaluiert um die gesamten Logs zentral zu speichern und abfragen zu können.


==== Reaktion

Da Abweichungen in der Baseline ein grosses Gefahrenpotential besitzen, muss klar definiert werden, wie auf eine Abweichung zu reagieren ist. +
{nbsp} +
Natürlich beinhaltet dieser Flowchart nur die grundlegendsten Aktivitäten. Genaueres vorgehen muss nach der Evaluation definiert werden.

image::flowchart.png[Flowchart einer Reaktion,align="center"]

<<<

=== Umsysteme

* **Prozess**
** Prozesse müssen respektiert werden.
** Prozesse, welche im Netzwerk aktiv sind, sollen weder beeinflusst noch abgeändert werden.

* **Richtlinien**
** Implementierte Richtlinien sollen weiterhin respektiert und befolgt werden.
** Müssen eventuell erweitert werden

=== Schnittstellen

* Entscheidung und Analyse bei gemeldeten incidents
* Kommunikation bei true positiv alerts
* Aktive Anpassung der Baseline
* Kommunikation bei Änderungen im Netzwerk


=== Systemgrenzen

* Analyse betrifft nur das POC LAN und darf nicht auf das produktive LAN ausgeweitet werden.


=== Gemeinsamkeiten
Alle Untersysteme müssen einen gewissen Grad an Compliance mit den gegebenen IST-Zuständen aufweisen können. Dies bedeutet, dass bei bereits eingesetzter Software/Hardware APIs zur Verfügung stehen müssen, um effektiv Anomalien erkennen zu können.

<<<

== Erhebung

=== IST-Zustand
Das POC LAN https://gitlab.com/nliechti/up2/wikis/home[@Gitlab] besitzt ein NMS (Network Monitoring System) welches bereits rudimentär den Status des Netzwerkes überwacht (Throughput). Anomalien können allerdings nicht näher erkannt werden, noch kann mit Genauigkeit gesagt werden, wo die Anomalie aufgetreten ist.

=== Quantitative Methode

==== Beobachtungen, Messungen

Um einen besseren Überblick über die vorhandene Infrastruktur zu bekommen, wird mithilfe einigen Tools Messungen und Beobachtungen anhand des POC Lans durchgeführt.
Die genaue Analyse der erhobenen Daten wird in der Hauptstudie genauer analysiert.
Dank der Übersichtlichen Wiki Darstellung im Gitlab, sind alle Informationen vorhanden.

<<<

=== Risiken
* [R1] "Shadow IT" führt zu Fehlkonfigurationen.
* [R2] Lateral movement wird nicht erkannt.
* [R3] Infektionen bleiben über längeren Zeitraum unerkannt.
* [R4] Rouge Systeme können im Netzwerk schaden anrichten.
* [R5] APT deployt einen persistent backdoor.
* [R6] Zugangsdaten und Informationen können geleakt werden.

==== Risikoanalyse

image::risikomatrix.png[Risikomatrix,640]

<<<

== Würdigung

=== IST-Zustand
Die Infrastruktur bietet eine Basis, um Ausfälle einzelner Dienste zu sehen und die Administratoren zu informieren. Paketverlust oder andere Anomalien können zzt. noch nicht erkannt werden.
Da bereits Switches, welche via SNMP überwacht werden vorhanden sind, kann darauf aufgebaut werden, weitere Informationen zu bekommen.

=== SWOT-Analyse
Die SWOT-Analyse soll Aufsicht über den momentanen Zustand geben. +
Mithilfe der Matrix wird erhofft zukünftige Chancen sowie momentane Schwächen besser feststellen zu können.

==== Rahmenbedingungen
* Firmen überwachen meist nur ihre Perimeter, nicht aber Traffik innerhalb der Segmente.
* Durchschnittlicher Lifecycle eines Databreaches beträgt 279 Tage https://newsroom.ibm.com/2019-07-23-IBM-Study-Shows-Data-Breach-Costs-on-the-Rise-Financial-Impact-Felt-for-Years#assets_all[IBM]
* Erst nach 207 Tagen, wird im Durchschnitt ein Einbruch gefunden.
* Firmen setzen häufig auf Endpoint-Security und Firewalls. Allerdings nicht auf HIDS/NIDS

==== Stärken/Schwächen Profil

.*Stärken*
Auch wenn innerhalb des POC LANs kein Mechanismus vorhanden ist um Anomalien zu erkennen, sind dennoch die rudimentären Anforderungen vorhanden um solch ein System einzurichten. Ebenfalls besteht bereits eine Perimeter-Überwachung welche die wichtigsten Dienste, welche ein fehlerfreies Arbeiten garantiert. Da der grösste Teil der Infrastruktur auf Open Source basiert, ist es ebenfalls ein leichtes, Module zu erweitern und anzupassen.

.*Schwächen*
Die Umgebung an sich besitzt ein unzureichendes Monitoring. Anomalien können nur schlecht oder gar nicht erkannt werden. Im Falle einer entdeckten Anomalie, kann aufgrund fehlender zentralen Loggings nicht garantiert werden, dass der Verursacher gefunden werden kan.

.*Chancen*
Die Software für die Anomalie Erkennung soll einen Mehrwert in der gesamten Struktur des Netzwerkes erbringen. Allgemeine Risiken und Gefahren können durch ein frühzeitiges erkennen eingedämmt oder direkt unterbunden werden. Durch den Einsatz eines Log-Collectors und eines NMS, können Anhaltspunkte zur Überwachung des Netzwerks gegeben werden.

.*Gefahren*
Durch die erhöhte Überwachung der Netzwerke könnte es zu Datenschutz verletzungen führen. Sowie erhöhter Administrativer Aufwand die Baseline zu wahren.

==== Matrix

image::swot.png[SWOT-Analyse,align="center"]

<<<

== Lösungsvarianten
Um das bereits vorhanden POC LAN auszubauen und mit einem Anomalie Erkennungsmodul auzustatten, ist es Notwendig, einen zentralen Log-Collector, sowie ein richtig konfiguriertes NMS zu haben. Anhand der definierten Kriterien und Gewichtungen werden die Lösungen gegenübergestellt.

=== Log-Collector
[quote, techtarget.com]
_____
Log management is the collective processes and policies used to administer and facilitate the generation, transmission, analysis, storage, archiving and ultimate disposal of the large volumes of log data...
_____
Zentrales logging ist in Hinsicht auf ein Anomalie Erkennungstool beinahe unumgänglich. +
Anhand der Logs können folgende Dinge erkannt werden: +

- Zugriffsverletzung
- Passwortänderungen
- Neustarts
- Änderungen von Konfigurationen

Da beinahe (eine Ausnahme) nur GNU/Linux Server im Einsatz sind, muss der Log-Collector Syslog unterstützen um die Logs sammeln zu können. +
Um die Software besser bewerten zu können, wird jeweils eine virtuelle Maschine mit einer Instanz der Software installiert und analysiert.

image::monitor.png[Log-Management,640,400]

==== V0 | Null
Logs werden weiterhin ohne zentralen Server gemanagt. Dies würde dazu führen, dass Logs einzeln von den Devices abgeholt werden müssten.

===== Keypunkte

* Geringster initialer Aufwand.
* Höherer Aufwand in der Programmierung.
* Erhöht die Gefahr, Anomalien nicht zu erkennen.

===== Vorteile

.*Geringster initialer Aufwand*
Ohne Log-Collector müsste das Syslog bei keinem Server konfiguriert werden. Somit könnte man initial den Aufwand minimieren und die Zeit in andere Punkte investieren.

===== Nachteile

.*Höherer Programmieraufwand*
Die Logs können nicht mehr von einer zentralen Stelle abgefragt werden. Dies würde dazu führen, dass man eine Liste von vorhandenen Servern führen müsste und diese einzeln jeweils über ihren momentan Stand abfragen.

.*Anomalien nicht erkennen*
Zentrales Log Management erhöht die Durchsicht in einem Netzwerk immens. Ohne Log-Collector können einzelne Server/Dienste vergessen gehen oder nicht abgefragt werden.

<<<

==== V1 | Graylog

[cols=".<7,>3",grid="None",frame="None",align="center",width="80%"]
|====
|Graylog ist nicht nur eine Log Management Lösung. Sondern beinhaltet ein komplettes SIEM. Der unterliegende Storage welcher auf Elasticsearch basiert, ermöglicht eine schnelle Suche der Dateien. Besonders interessant ist das REST API und ihre OpenSource Lizenz welche Graylog kostengünstig und erweiterbar macht.
a|image:graylog.jpg[width="250px"]
|====

===== Keypunkte

* Open Source und Enterprise Model
* Free (Enterprise bis 5GB/Day)
* RESTful
* _all-or-nothing_ solution
* Limited scope

===== Vorteile

.*Free*
Das Open Source sowie Enterprise Model sind von Interesse wenn es darum geht, Logs zusammenzuführen und abzuarbeiten. Falls es zu einer Entscheidung zu Graylog kommen sollte, wird allerdings auf das Enterprise Model verzichtet, da es sich nur um ein POC handelt.

.*RESTful*
Das integrierte REST API ermöglicht es, selbst Wrapper für die Applikation zu schreiben, um schnell an die gewünschten Informationen zu kommen.

===== Nachteile

.*All-Or-Nothing*
Graylog kann eines gut - Logs managen. +
Andere Lösungen bieten die Funktionalität direkt Graphen aus den Logs herauszuschreiben. Graylog hingegen benötigt dafür weitere Tools (Graphana).

.*Limited scope*
Wie oben genannt, bietet Graylog die Möglichkeit nicht an, Logs in andere Daten umzuwandeln um sie Beispielsweise in den KPIs anzuzeigen.

<<<


==== V2 | ELK-Stack
[cols=".<6,>4",grid="None",frame="None",align="center",width="80%"]
|====
| Elasticsearch ist einer der Platzhirsche in Datenverarbeitung und so ziemlich alles, was mit Bigdata und Datenanalyse zu tun hat. Durch Kibana besitzt es eine relativ gute Weboberfläche und mit Logstash besitzt es die Möglichkeit Logs zu empfangen und zu bearbeiten.
a|image:elk.png[width="350px"]
|====

===== Keypunkte

* Vollumfänglich
* Clients senden via Beats Logs zum Server
* RESTful
* Hohe Lernkurve
* Hohe Wartungskosten
* Logtransformierung
* Keine Authentifizierung in der Free-Version
* Aufteilung via Shards

===== Vorteile
.*Vollumfänglich*
Der ELK-Stack beinhaltet alles, was man zum Log Managen braucht. Via Beats können Logs von Windows sowie Linux Client empfangen und transformiert werden. Kibana ermöglicht das Anzeigen der Logs in Realtime direkt in einer Weboberfläche und Elasticsearch bietet eine vollumfängliche analytische Umgebung, um Logs zu analysieren an.

.*Beats*
Dadurch das eine Software genutzt wird, um Logs aus den Clients zum Server zu senden (würde auch via Syslog gehen), können zusätzliche Informationen angehängt werden. Zum Beispiel ermöglicht dies dem Server mitzuteilen, welche Beats Version momentan auf dem Client läuft.

<<<

.*RESTful*
Das integrierte REST API ermöglicht es, selbst Wrapper für die Applikation zu schreiben, um schnell an die gewünschten Informationen zu kommen.

===== Nachteile
.*Hohe Wartungskosten*
Die Erfahrung mit ELK zeigt, dass die Kosten der Wartung des Servers ziemlich hoch sind (Zeitkosten). Falls ein Shard fehlschlagen sollte, ist eine Rückführung der defekten Logs extrem aufwändig bis beinahe unmöglich. Dazu kommt, dass Beats nicht mit allen Logtypen umgehen kann und unter gewissen Umständen eigene Parser geschrieben werden müssen.

.*Hohe Lernkurve*
Die Administration eines ELK-Stack ist nicht einfach. Bereits das Verwalten von mehreren Nodes ist extrem zeitaufwändig, geschweige von den Konfigurationsmöglichkeiten welche Logstash mit sich bringt.

.*Keine Authentifizierung*
Die Community Version des ELK-Stacks (Kibana) bietet keine Authentifizierung an. Das heisst, dass zusätzlich ein reverse Proxy eingesetzt werden müsste, welcher die Authentifizierung übernimmt.

<<<

==== V3 | Splunk

[cols=".^7,^3",grid="None",frame="None",width="80%",align="center"]
|====
| Splunk sollte jedem welcher sich mit Log Management etwas beschäftigt hat ein Begriff sein. Besonders interessant ist Splunk Enterprise Security (ES) welches als SIEM von Splunk selbst dient. Es bietet ein weites Modulares System durch Community erstellte Apps bereit.
a| image:splunk.png[width="190px"]
|====

===== Keypunkte

* Eigene Query Sprache (SPL)
* Standard APIs
* Easy Setup
* Monitoring und Alerting
* Kein Java
* Properitär

===== Vorteile
.*Standard APIs*
Wie bei den restlichen Tools wird ein RESTful sowie HTTP API verwendet.

.*Monitoring und Alerting*
Splunk besitzt bereits selbst die Funktionalität, Daten zu überwachen und bei Sonderfällen die Administratoren zu informieren.

.*Easy Setup*
Das Setup benötigt nur einen Splunk Server und evtl. einen Splunk Forwarder. Zusätzliche Komponenten wie bei den anderen Lösungen werden nicht benötigt.

.*Kein Java*
Es lebt sich leichter mit Server welche kein Java benötigten. Splunk ist in C++ / Python geschrieben und benötigt dadurch keine JVM oder zusätzliche Software.

===== Nachteile

.*Properitär*
Splunk ist nicht Open Source. Dies behält den Nachteil, dass man nicht sicher gehen kann, was die Software genau unterliegend macht. Im Fehlerfall, kann nicht direkt im Code nachgeschaut werden, was nun schiefgelaufen ist.

==== V-Bewertung

image::nutzwert.png[Nutzwertanalyse]

===== Analyse
Grundsätzlich war es ein sehr enges Rennen. +
Bei einem ELK-Stack sind die Versionen etwas ein Problem. Viele Tools laufen nur mit älteren Versionen von Elasticsearch und sind daher auf bestimmte Konfigurationen angewiesen. +
Graylog hingegen würde ich gerne anderswo nochmals testen. Grundsätzlich kann man allerdings sagen, das Splunk eines der weitverbreitesten und innovativsten Log Monitoring Managements ist. Daher fiel die Wahl u. a. der grossen Community und ausführliche Dokumentation auf Splunk.

<<<


=== Network Monitoring System

Netzwerk Monitoring ist ein systematischer versuch, fehlerhafte Komponenten zu entdecken und den Administrator darauf aufmerksam zu machen. +
Die Funktion, fehlerhafte Komponente zu finden, wird in diesem Projekt nur zweitrangig angeschaut. Wichtiger ist die Funktion, allgemeine Informationen aus den Netzwerk-Komponenten zu ziehen und sie zusammen mit anderen Informationen zu deuten.

{nbsp} +
{nbsp} +
{nbsp} +
{nbsp} +

image::nms.png[Network Monitoring System,width="640"]
<<<

==== V0 | Null

Es wird kein NMS verwendet. +
Das Monitoring wird Manuell gehandhabt.

===== Keypunkte

* Geringster initialer Aufwand.
* Höherer Aufwand in der Programmierung.
* Erhöht die Gefahr, Anomalien nicht zu erkennen.
* Downtime kann nicht erkannt werden.

===== Vorteile

.*Geringster initialer Aufwand*
Durch die nichtbenutzung eines NMS benötigt es keine zusätzliche Konfigurationen.

===== Nachteile

.*Höherer Programmieraufwand*
Die Logs können nicht mehr von einer zentralen Stelle abgefragt werden. Dies würde dazu führen, dass man eine Liste von vorhandenen Server führen müsste und diese einzeln jeweils über ihren momentanen Stand abfragen.

.*Anomalien nicht erkennen*
Zentrales Log Management erhöht die Übersicht in einem Netzwerk immens. Ohne Log-Collector können einzelne Server/Dienste vergessen gehen oder nicht abgefragt werden.

<<<

==== V1 | PRTG

[cols=".^7,>3",width="80%",align="center",grid="None",frame="None"]
|====
| Obwohl PRTG eine proprietäre Monitoring Lösung ist, findet es immer wieder Erwähnung, wenn es um stabile und umfassende NMS geht. +
PRTG arbeitet mit sogenannten Sensoren, um von einem zentralem Windows Server aus, jegliche Informationen zu aggregieren.
a|image:prtg.png[width="250px"]
|====

===== Architektur

image::prtg_overview.png[PRTG Overview,640,400,align="center"]

<<<

===== Keypunkte

* Benötigt einen Windows-Server
* Free Version beinhaltet maximal 100 Sensoren
* Anbindung an OpenLDAP nicht möglich
* Leichte Überwachung neuer Dienste
* Hohe Anzahl an bereits vorhandenen Sensoren
* Bereits im POC vorhanden

===== Vorteile

.*Simple Konfiguration*
Hinzufügen neuer Sensoren & Services funktioniert direkt auf Knopfdruck. Konfiguration kann anhand eines gut übersichtlichen Webinterfaces vorgenommen werden.
Die Handhabung im Sinne der Komplexität unterscheidet sich massgebend von den restlichen NMS.

.*Vorhandene Installation*
Im NWD Projekt wurde bereits ein PRTG aufgesetzt und konfiguriert. Dies bedeutet, dass keine zusätzlichen Anpassungen gemacht werden müssten.

===== Nachteile

.*Windows*
Da der Masterserver von PRTG zwingend eine Windows Umgebung benötigt, ist es unumgänglich einen einzelnen Windows Server zu betreiben.

.*OpenLDAP*
PRTG kann nicht offiziell via OpenLDAP angebunden werden. Dies bedeutet, dass die User und Berechtigungen zusätzlich verwaltet werden müssten.

<<<

==== V2 | Nagios
[cols=".^7,.>3",grid="None",frame="None",width="80%",align="center"]
|====
| Das im Jahre 2002 Entwickelte NMS Nagios ist eines der führenden Monitoring Lösungen in Linux Umgebungen. Mithilfe diversen Plugins / Agenten lassen sich auch andere Plattformen überwachen und liefern ihre Daten an einen lokalen Nagios Server.
a|image:nagios.png[width="250px"]
|====

===== Architektur

image::nagios_overview.png[Nagios Overview,640,400,align="center"]

<<<

===== Keypunkte

* Läuft auf *nix
* Eigene Syntax/Sprache
* Viele Plugins
* OpenSource
* Eher höhere Lernkurve
* Ohne Scripts etwas umständlich

===== Vorteile

.**NIX*
Dadurch dass Nagios unter Linux läuft, benötigt es keinen zusätzlichen Windows Server in einer sonst homogenen Infrastruktur.

.*Plugins*
Mithilfe der Plugins ist es möglich auch eher fremdartige Systeme zu überwachen. Viele diese Plugins sind offiziell unterstützt und werden aktiv durch die Community entwickelt.

===== Nachteile

.*Komplexität*
Die Konfiguration neuer Services benötigt einiges an Zeit. Da das Ziel des Projektes nicht eine Inbetriebnahme eines NMS ist, befindet sich die Komplexität ausserhalb des Scopes.

.*Zusätzliche Systeme*
Eine Installation müsste im POC LAN komplett vorgenommen werden. Es gibt noch keinen Server welcher frei für eine NMS installation ist.

<<<

==== V-Bewertung

image::nutzwert_nms.png[Nutzwertanalyse]

==== Analyse
Da PRTG bereits im POC Netzwerk konfiguriert ist und alle notwendigen Punkte erfüllt werden konnten, viel die Wahl relativ einfach. +
Nagios an sich lohnt sich, wenn man ein neues NMS einführen muss, welches möglichst hohe anpassbarkeit benötigt. Zusätzlich dazu, ist die Lizenz welche Nagios bietet, weniger restriktiv als die von PRTG.
Die APIs der beiden Lösungen, verhalten sind relativ gleich. Nagios hätte noch zusätzlich die Möglichkeit, RESTful via Module zu unterstützen, allerdings ist dies nicht dringend notwendig.


<<<

== Weiteres Vorgehen
Anhand der nun evaluierten Software, kann mit der Hauptstudie fortgefahren werden. In der Hauptstudie wird genauer auf die Applikation eingegangen und bereits erste Ansätze des Designs entschieden. Da die Umsysteme alle über APIs verfügen, muss evaluiert werden, ob bereits Module für die Abfrage vorhanden sind. Falls dies nicht der Fall sein sollte, müssen weitere Module entwickelt werden. 

Das POC Lan wird mit den evaluierten Technologien aufgewertet und miteinander verknüpft. Danach kann angefangen werden, die einzelnen Systeme abzufragen und die Auswertungen miteinander zu kombinieren.



<<<

== Freigabe

{nbsp} +
{nbsp} +

.*Auftraggeber*
Technische Berufsschule Zürich +
Sihlquai 101 +
8090 Zürich +
admin.hf@tbz.zh.ch

{nbsp} +

.*Projektleitung*
{nbsp} +
{nbsp} +
Ives Schneider +
Binzstrasse 19 +
8712 Stäfa +
ives.schneider@i-401.xyz

{nbsp} +

.*Experte*
{nbsp} +
{nbsp} +
Marco Sieber +
marco.sieber@tbz.ch

<<<

== Darstellungsverzeichnis

[cols="<,>",width="80%",align="center",frame="none",grid="none"]
|=====
| 1
| Projekt Mitarbeiter
| 2
| Präferenzmatrix
| 3
| Systemgrenzen
| 4 
| Flowchart einer Reaktion
| 5
| Risikomatrix
| 6
| SWOT-Analyse
| 7
| Log-Management
| 8
| Graylog
| 9
| ELK
| 10
| Splunk
| 11
| Nutzwertanalyse
| 12
| Network Monitoring System
| 13
| PRTG
| 14
| PRTG Overview
| 15
| Nagios
| 16
| Nagios Overview
| 17
| Nutzweranalyse
|=====


<<<

== Glossar

.NMS
Network Monitoring System - Überwachungssystem

.POC
Proof of concept - Konzeptbeweise

.Shadow IT
Unbekannte Geräte innerhalb der Unternehmung.
